#!/bin/bash
#SBATCH --job-name=do_task       # create a name for your job
#SBATCH --nodes=1                # node count
#SBATCH --ntasks=24              # total number of tasks
##SBATCH --exclusive
#SBATCH --cpus-per-task=1        # cpu-cores per task
#SBATCH --time=06:00:00          # total run time limit (HH:MM:SS)
#SBATCH --mem=1500G
#SBATCH --partition=bigmem

module purge
# module load anaconda3/2020.11 openmpi/gcc/4.1.0
# conda activate pyathena
module load anaconda3/2022.5 openmpi/gcc/4.1.0
conda activate pyathena-lem

MODULE="pyathena.tigress_ncr.do_tasks"
echo $PYTHONPATH
export PYTHONPATH="$HOME/pyathena"
echo $PYTHONPATH

OUTDIR=$1

cd $HOME/pyathena
# srun mprof run --multiprocess -o scripts/logs/mprof-$SLURM_JOBID.dat
srun python pyathena/tigress_ncr/do_tasks.py -b $OUTDIR 1> scripts/logs/do_tasks-$SLURM_JOB_ID.out 2> scripts/logs/do_tasks-$SLURM_JOB_ID.err
# mprof plot scripts/logs/mprof-$SLURM_JOBID.dat -o scripts/logs/mprof-$SLURM_JOB_ID.png -f
# srun python pyathena/tigress_ncr/do_tasks_surfmaps.py -b $OUTDIR 1> scripts/do_surfmaps-$SLURM_JOB_ID.out 2> scripts/do_surfmaps-$SLURM_JOB_ID.err
#srun python pyathena/tigress_ncr/do_tasks_chbreak.py -b $OUTDIR 1> scripts/do_tasks-$SLURM_JOB_ID.out 2> scripts/do_tasks-$SLURM_JOB_ID.err
